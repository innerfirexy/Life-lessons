{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from transformers import BertModel, BertTokenizer\n",
    "import torch\n",
    "from typing import List, Any\n",
    "from tqdm import tqdm\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "data": {
      "text/plain": "Downloading:   0%|          | 0.00/232k [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "577d69a346d14450a8c275355b7271d0"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Downloading:   0%|          | 0.00/28.0 [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "86cfc15c80974983945f96d6aa4b99d1"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Downloading:   0%|          | 0.00/570 [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "ff44baab23024a33aa2d868cd02af16b"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Downloading:   0%|          | 0.00/440M [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "558de461ac41405f87082b75f84a4721"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertModel.from_pretrained('bert-base-uncased')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "data_dir = '../data/mixed/'\n",
    "train_file = 'train.raw.txt'\n",
    "test_file = 'test.raw.txt'\n",
    "\n",
    "with open(data_dir + train_file, 'r') as f:\n",
    "    train_data = f.readlines()\n",
    "with open(data_dir + test_file, 'r') as f:\n",
    "    test_data = f.readlines()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "casting director\t70 50\n",
      "and getting paid for it so let's get\t40 67 68 68 68 41 67 41\n",
      "ways that i connect with you guys is so\t79 79 79 70 70 79 76 79 79\n",
      "different varieties and if you don't\t50 23 22 19 19 19\n",
      "along with like twice a day coworking events on zoom\t59 58 58 58 58 58 58 58 58 58\n",
      "doing both\t79 79\n",
      "while investing in real estate the first\t44 53 53 44 44 44 43\n",
      "so if you're interested in learning\t67 71 71 71 71 71\n",
      "really makes a difference so with that\t40 40 40 40 40 13 22\n",
      "you don't want to rain just like peace\t49 49 49 49 49 49 49 49\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    print(train_data[i][:-1])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['casting', 'director']\n",
      "tensor([[9179, 2472]])\n",
      "torch.Size([1, 2, 768])\n",
      "torch.Size([1, 768])\n",
      "torch.Size([768])\n"
     ]
    }
   ],
   "source": [
    "line = train_data[0][:-1]\n",
    "words, gestures = line.split('\\t')\n",
    "\n",
    "tokens = tokenizer.tokenize(words)\n",
    "print(tokens)\n",
    "input_ids = torch.tensor([tokenizer.encode(tokens, add_special_tokens=False)])\n",
    "print(input_ids)\n",
    "embeddings = model(input_ids)[0]\n",
    "print(embeddings.shape)\n",
    "\n",
    "word_spans = [(0,1), (1,2)]\n",
    "print(embeddings[0, word_spans[0][0]:word_spans[0][1], :].shape)\n",
    "print(embeddings[0, word_spans[1][0]:word_spans[1][1], :].mean(dim=0).shape)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5984 17739\n",
      "and getting paid for it so let's get\t40 67 68 68 68 41 67 41\n"
     ]
    }
   ],
   "source": [
    "# Check whether the lengths of words and gestures are the same for all lines\n",
    "count = 0\n",
    "nomatch_lines = []\n",
    "for i, line in enumerate(train_data):\n",
    "    line = line[:-1]\n",
    "    words, gestures = line.split('\\t')\n",
    "    input_ids = torch.tensor([tokenizer.encode(words, add_special_tokens=False)])\n",
    "    num_ids = input_ids.shape[1]\n",
    "    if num_ids != len(gestures.split()):\n",
    "        nomatch_lines.append(i)\n",
    "        count += 1\n",
    "        # print(line)\n",
    "print(count, len(train_data))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "and getting paid for it so let's get\t40 67 68 68 68 41 67 41\n",
      "[1998, 2893, 3825, 2005, 2009, 2061, 2292, 1005, 1055, 2131]\n",
      "<class 'list'>\n",
      "40 67 68 68 68 41 67 41\n"
     ]
    }
   ],
   "source": [
    "print(train_data[nomatch_lines[0]][:-1])\n",
    "words, gestures = train_data[nomatch_lines[0]][:-1].split('\\t')\n",
    "input_ids = tokenizer.encode(words, add_special_tokens=False)\n",
    "print(input_ids)\n",
    "print(type(input_ids))\n",
    "print(gestures)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [],
   "source": [
    "# for lines where the lengths of words and gestures are not the same\n",
    "# estimate the word spans for the corresponding gestures\n",
    "# For example, for input_ids = [1998, 2893, 3825, 2005, 2009, 2061, 2292, 1005, 1055, 2131], whose length is 10\n",
    "# the corresponding gestures are '40 67 68 68 68 41 67 41', whose length is 8,\n",
    "# then the word span for the first gesture 40 is [0,1], where 0 is from round(10*(0/8))\n",
    "# For the second gesture 67, the word span is [1,2], where 1 is from round(10*(1/8))\n",
    "# For the third gesture 68, the word span is [2,4], where 2 is from round(10*(2/8)) and 4 is from round(10*(3/8))\n",
    "# For the fourth gesture 68, the word span is [5,6], where 5 is from round(10*(3/8)) and 6 is from round(10*(5/8))\n",
    "# For the fifth gesture 68, the word span is [7,8], where 7 is from round(10*(6/8)) and 8 is from round(10*(7/8))\n",
    "# Gesture => Word span\n",
    "# 40, [0,1] => 10*(0/7)=0, 0+1=1 => [0,1]\n",
    "# 67, [1,2] => left = round(10*(1/8))=1, right = max(left+1, round(10*(2/8)))=2 => [1,2]\n",
    "# 68, [2,3] => left = round(10*(2/8))=2, right = max(left+1, round(10*(3/8)))=4 => [2,4]\n",
    "# 68, [3,4] => left = round(10*(3/8))=4, right = max(left+1, round(10*(4/8)))=5 => [4,5]\n",
    "# 68, [4,5] => left = round(10*(4/8))=5, right = max(left+1, round(10*(5/8)))=6 => [5,6]\n",
    "# 41, [5,6] => left = round(10*(5/8))=6, right = max(left+1, round(10*(6/8)))=8 => [6,8]\n",
    "# 67, [6,7] => left = round(10*(6/8))=8, right = max(left+1, round(10*(7/8)))=9 => [8,9]\n",
    "# 41, [7,8] => left = round(10*(7/8))=9, right = max(left+1, round(10*(8/8)))=10 => [9,10]\n",
    "\n",
    "def get_word_spans(input_ids: List[Any], gestures: List[Any]):\n",
    "    num_ids = len(input_ids)\n",
    "    num_gestures = len(gestures)\n",
    "    word_spans = []\n",
    "    for i in range(num_gestures):\n",
    "        left = round(num_ids * (i / num_gestures))\n",
    "        right = max(left + 1, round(num_ids * ((i + 1) / num_gestures)))\n",
    "        word_spans.append((left, right))\n",
    "    return word_spans"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 1), (1, 2), (2, 4), (4, 5), (5, 6), (6, 8), (8, 9), (9, 10)]\n"
     ]
    }
   ],
   "source": [
    "# Test get_word_spans\n",
    "input_ids = [1998, 2893, 3825, 2005, 2009, 2061, 2292, 1005, 1055, 2131]\n",
    "gestures = '40 67 68 68 68 41 67 41'.split()\n",
    "word_spans = get_word_spans(input_ids, gestures)\n",
    "print(word_spans)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 17739/17739 [10:07<00:00, 29.22it/s]\n"
     ]
    }
   ],
   "source": [
    "gesture_embeds = {}\n",
    "for data in tqdm(train_data):\n",
    "    line = data[:-1]\n",
    "    words, gestures = line.split('\\t')\n",
    "    input_ids = tokenizer.encode(words, add_special_tokens=False)\n",
    "    gestures = gestures.split()\n",
    "    word_spans = get_word_spans(input_ids, gestures)\n",
    "    # get embeddings\n",
    "    input_ids_pt = torch.tensor([input_ids])\n",
    "    embeddings = model(input_ids_pt)[0]\n",
    "    for i, gesture in enumerate(gestures):\n",
    "        if gesture not in gesture_embeds:\n",
    "            gesture_embeds[gesture] = []\n",
    "        gesture_embeds[gesture].append(embeddings[0, word_spans[i][0]:word_spans[i][1], :].mean(dim=0))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "86\n",
      "9032\n",
      "torch.Size([768])\n",
      "['71', '13', '70', '46', '63', '10', '81', '54', '60', '48', '5', '7', '31', '-59', '61', '3', '45', '-29', '12', '78', '36', '19', '56', '39', '76', '58', '17', '67', '2', '33', '53', '-119', '59', '62', '69', '64', '27', '14', '77', '68', '65', '50', '35', '-159', '11', '4', '75', '37', '-89', '25', '26', '72', '47', '80', '74', '38', '41', '44', '-179', '51', '79', '43', '23', '18', '22', '73', '0', '55', '24', '52', '32', '15', '42', '34', '20', '6', '1', '30', '49', '21', '-149', '-239', '40', '28', '16', '29']\n",
      "max gesture token: 81\n",
      "invalid gesture tokens: {-159, -29, -59, -89, -119, -149, -179, -239}\n"
     ]
    }
   ],
   "source": [
    "print(len(gesture_embeds))\n",
    "print(len(gesture_embeds['40']))\n",
    "print(gesture_embeds['40'][0].shape)\n",
    "print(list(gesture_embeds.keys()))\n",
    "\n",
    "# max gesture token\n",
    "gesture_token_int = list(map(int, gesture_embeds.keys()))\n",
    "print('max gesture token:', max(gesture_token_int))\n",
    "print('invalid gesture tokens:', set(gesture_token_int) - set(range(max(gesture_token_int) + 1)))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "outputs": [],
   "source": [
    "# Save the gesture embeddings\n",
    "with open('gesture_embeds.pkl', 'wb') as f:\n",
    "    pickle.dump(gesture_embeds, f)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "outputs": [],
   "source": [
    "# t-SNE plot for gesture embeddings\n",
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import random"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# plot the t-SNE plot for the first 1000 gestures\n",
    "num_gestures = 1000\n",
    "\n",
    "print(gesture_embeds.shape)\n",
    "tsne = TSNE(n_components=2, random_state=0)\n",
    "gesture_embeds_2d = tsne.fit_transform(gesture_embeds)\n",
    "print(gesture_embeds_2d.shape)\n",
    "plt.scatter(gesture_embeds_2d[:, 0], gesture_embeds_2d[:, 1])\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
